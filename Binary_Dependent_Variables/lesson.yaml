- Class: meta
  Course: Econometrics
  Lesson: Binary Dependent Variables
  Author: Nick Huntington-Klein
  Type: Standard
  Organization: Seattle University
  Version: 2.4.5

- Class: text
  Output: |-
   This lesson will go through the probit and logit regression methods, used when your dependent variable is binary (0/1, False/True, No/Yes, etc), as well as generalized linear models generally.
   
   For this lesson you will need the marginaleffects, fixest, tidyverse, and vtable packages. Also make sure the car package is installed, although we won't be loading it.
   
   You can install these by exiting out of here (bye() works), installing these with install.packages(c('marginaleffects','fixest','tidyverse','vtable','car'), dependencies = TRUE), and coming back in with swirl().

- Class: cmd_question
  Output: |-
   Now let's load all the packages! 
   
   Do library(tidyverse)
  CorrectAnswer: library(tidyverse)
  AnswerTests: omnitest(correctExpr='library(tidyverse)')
  Hint: Copy in library(tidyverse)

- Class: cmd_question
  Output: |-
   Now library(vtable)
  CorrectAnswer: library(vtable)
  AnswerTests: omnitest(correctExpr='library(vtable)')
  Hint: Copy in library(vtable)
 
- Class: cmd_question
  Output: |-
   Now library(fixest)
  CorrectAnswer: library(fixest)
  AnswerTests: omnitest(correctExpr='library(fixest)')
  Hint: Copy in library(fixest)
 
- Class: cmd_question
  Output: |-
   Now library(marginaleffects)
  CorrectAnswer: library(marginaleffects)
  AnswerTests: omnitest(correctExpr='library(marginaleffects)')
  Hint: Copy in library(marginaleffects)

- Class: cmd_question
  Output: Now let's load in our data. Load in the TitanicSurvival data set, which comes from the carData package. Since we haven't actually loaded the carData package, we can load the data directly by adding a package='carData' option to our data() function. This data set tracks the demographics and survival of passengers aboard the Titanic ship disaster.
  CorrectAnswer: data(TitanicSurvival, package = 'carData')
  AnswerTests: omnitest(correctExpr='data(TitanicSurvival, package = "carData")')
  Hint: Use TitanicSurvival in data()

- Class: cmd_question
  Output: Let's take a look at the data. First, use vtable() to look at the variables and variable types in the data. Include the lush = TRUE option.
  CorrectAnswer: vtable(TitanicSurvival, lush = TRUE)
  AnswerTests: any_of_exprs('vtable(TitanicSurvival, lush = TRUE)','vt(TitanicSurvival, lush = TRUE)','TitanicSurvival %>% vtable(lush = TRUE)','TitanicSurvival %>% vt(lush = TRUE)')
  Hint: Send TitanicSurvival to vtable(lush = TRUE)

- Class: text
  Output: |-
   Notice a few things here:
   
   1. Only one of the variables - age - is numeric, and it runs from .167 to 80
   
   2. The others are factor variables - categorical.
   
   3. Two of the factor variables are binary - survived and sex
   
   4. The factors are ordered ('no' then 'yes' and 'female' then 'male' and '1st' '2nd' and '3rd'). If we use an ordered factor in regression, it will omit the first category and use the others. So if we use survived as a dependent variable, it will have 'yes' = TRUE/1 since that comes second. Or if we use passengerClass as an independent variable, the '1st' category will be omitted and we'll get binary variables for '2nd' and '3rd'. Refer back to the binary variables lessons!
   
   5. We have a fair bit of missing age data


- Class: cmd_question
  Output: |-
   That missing age data will mess up our code later, since the number of predicted values won't match the number of observations in the data.
   
   Use 
   
   TitanicSurvival <- TitanicSurvival %>% na.omit()
   
   to drop any rows with missing data in any variable.
   
   This isn't necessarily the most principled way to deal with missing data, but it will do for now!
  CorrectAnswer: TitanicSurvival <- TitanicSurvival %>% na.omit()
  AnswerTests: omnitest(correctExpr='TitanicSurvival <- TitanicSurvival %>% na.omit()')
  Hint: Copy in the TitanicSurvival <- TitanicSurvival %>% na.omit() code


- Class: mult_question
  Output: |-
   We will be predicting survival with the other variables in the data.
   
   Let's start with a linear probability model. We're going to ue feols() to regress survival on the other variables in the data.
   
   Then, after we estimate it, we're going to be sure to *always* use it with robust standard errors (se = 'hetero' in feols or etable).
   
   Why is it important to always run a linear probability model with heteroskedasticity-robust standard errors?
  AnswerChoices: Because the outcome is always 0 or 1, error variance will always be higher when the prediction is near .5 than when it is near 0 or 1; Because we should always run robust standard errors regardless of anything; Because the LPM model predicts outside the bounds of 0 and 1; Because it helps handle endogeneity
  CorrectAnswer: Because the outcome is always 0 or 1, error variance will always be higher when the prediction is near .5 than when it is near 0 or 1
  AnswerTests: omnitest(correctVal='Because the outcome is always 0 or 1, error variance will always be higher when the prediction is near .5 than when it is near 0 or 1')
  Hint: Try drawing a picture with a bunch of observations at Y = 0 and at Y = 1, then draw a straight line connecting them. Then imagine what the variance of the points around the line is at a given X value.


- Class: cmd_question
  Output: |-
   Before we run our LPM, we need a slight adjustment.
   
   While the probit and logit models we're about to run are used to factors and can handle them, OLS can't! It needs the dependent variable to be a logical (TRUE/FALSE) or numeric (1/0).
   
   So, use mutate to create a new variable surv_TF equal to survived == 'yes' and overwrite TitanicSurvival.
  CorrectAnswer: TitanicSurvival <- TitanicSurvival %>% mutate(surv_TF = survived == 'yes')
  AnswerTests: cor(TitanicSurvival[['surv_TF']], TitanicSurvival[['survived']] == 'yes') > .99
  Hint: Use mutate as normal. This time you're creating a new variable surv_TF equal to survived == 'yes'


- Class: cmd_question
  Output: |-
   Okay! Now use feols() to regress surv_TF on the other variables in the data. Leave out se = 'hetero' for now, we'll add it in etable.
   
   Save the result as lpm
  CorrectAnswer: lpm <- feols(surv_TF ~ sex + age + passengerClass, data = TitanicSurvival)
  AnswerTests: ifelse(exists('lpm'),cor(predict(lpm),predict(feols(surv_TF ~ sex + age + passengerClass, data = TitanicSurvival))) > .99, FALSE)
  Hint: Run a feols regression as normal, using surv_TF as the dependent variable and sex, age, and passengerClass as independent.

- Class: cmd_question
  Output: Use etable() with se = 'hetero' to look at the result.
  CorrectAnswer: etable(lpm, se = 'hetero')
  AnswerTests: omnitest(correctExpr='etable(lpm, se = "hetero")')
  Hint: Send lpm to etable() with the se = 'hetero' option.


- Class: figure
  Output: |-
   Take a look at this figure graphing the LPM predicted values against the local means, with age as the x-axis variable.
   
   This graph was made using geom_smooth (and also y=surv_TF*1, since ggplot doesn't handle logicals properly with geom_smooth, it needs numbers)
   
   ggplot(TitanicSurvival, aes(x = age, y = surv_TF*1)) + 
      geom_point() + 
      geom_smooth(se = FALSE, color = 'blue') + 
      geom_smooth(method = 'lm', se = FALSE, color = 'red')
  Figure: lpm_local.R
  FigureType: new

- Class: mult_question
  Output: |-
   LPM seems to do a decent job here of matching the slope described by the local means, and never predicting outside the bounds of 0 and 1.
   
   What features of this problem might contribute to LPM doing a good job here?
  AnswerChoices: The average of the dependent variable is not near 0 or 1; LPM performs better than other methods when lots of fixed effects are involved; We're only interested in the slope, not the prediction; LPM is not doing a good job here.
  CorrectAnswer: The average of the dependent variable is not near 0 or 1
  AnswerTests: omnitest(correctVal='The average of the dependent variable is not near 0 or 1')
  Hint: The biggest differences between LPM and probit/logit come when the mean of the dependent variable is near 0 or 1.

- Class: cmd_question
  Output: |-
   Now let's estimate the model again using logit.
   
   We'll be using logit throughout the rest of this lesson, but everything could just as easily been done with probit with little difference.
   
   Use feglm() from fixest instead of feols(), and specify the option family = binomial(link = 'logit') to run our model using logistic regression. Save the result as logit.
   
   (by the way, exploring help(feglm) or help(glm) and its family option is a great way to learn about all sorts of GLM model link functions and what they're used for! There's way more than just linear and binary variables in the world! Even more than GLM covers!)
  CorrectAnswer: logit <- feglm(surv_TF ~ age + sex + passengerClass, data = TitanicSurvival, family = binomial(link = 'logit'))
  AnswerTests: ifelse(exists('logit'), cor(predict(logit), predict(feglm(surv_TF ~ age + sex + passengerClass, data = TitanicSurvival, family = binomial(link = 'logit')))) > .95, FALSE)
  Hint: Run the model just like a regular feols(), except use the feglm() function, and add the argument family = binomial(link = 'logit').

- Class: cmd_question
  Output: Use etable() with se = 'hetero' to look at lpm and logit together.
  CorrectAnswer: etable(lpm, logit, se = "hetero")
  AnswerTests: any_of_exprs('etable(lpm, logit, se = "hetero")','etable(logit, lpm, se = "hetero")')
  Hint: Send both lpm and logit to etable. Don't forget the se = 'hetero' option.

- Class: mult_question
  Output: How can we interpret the -.49 coefficient? Assume each option has "controlling for age and passenger class..." tacked onto the beginning.
  AnswerChoices: Men were 49% less likely to survive the Titanic; Men were 49 percentage points less likely to survive; Men were .49 more likely to survive the Titanic; There's no easy way to interpret the coefficient.
  CorrectAnswer: Men were 49 percentage points less likely to survive
  AnswerTests: omnitest(correctVal='Men were 49 percentage points less likely to survive')
  Hint: The interpretation of a coefficient in an LPM is the same as for regular OLS as if the dependent variable were "the probability that the dependent variable is 1".

- Class: mult_question
  Output: How can we interpret the -2.50 coefficient? Assume each option has "controlling for age and passenger class..." tacked onto the beginning.
  AnswerChoices: Men were 2.5% less likely to survive the Titanic; Men were 2.5 percentage points less likely to survive; Men were 250% less likely to survive; Men were 250 percentage points less likely to survive; Men were less likely to survive but it is hard to say how much; There's no easy way to interpret the coefficient.
  CorrectAnswer: Men were less likely to survive but it is hard to say how much
  AnswerTests: omnitest(correctVal='Men were less likely to survive but it is hard to say how much')
  Hint: Logit coefficients can be interpreted in logit terms but this is very difficult to do off-hand. However, we can get information from the sign (negative/positive) of the coefficient.

- Class: cmd_question
  Output: |-
   The way a GLM works is by using the coefficients to predict an *index variable*. Then it runs that index variable through the link function to get its predicted values.
   
   Use mutate to create a new variable called index_predict, overwriting the TitanicSurvival you have now.
   
   This variable should be based on predict(logit).
   
   However, we can't just say predict(logit) because now that we have a GLM, there are many things to predict - response probability, index itself, something else?
   
   So, add to predict(logit) the argument type = 'link' to get the index variable.
  CorrectAnswer: TitanicSurvival <- TitanicSurvival %>% mutate(index_predict = predict(logit, type = 'link'))
  AnswerTests: cor(TitanicSurvival[['index_predict']],predict(logit, type = 'link')) > .99; expr_uses_func('mutate')
  Hint: predict(logit, type = 'link') will give you the predicted values. Just make that the right-hand-side part of a mutate command creating the variable index_predict, storing that new variable in TitanicSurvival.

- Class: cmd_question
  Output: |-
   Now let's get its predictions for the probability that survived == 'yes'.
   
   Use mutate to create a new variable called response_predict, overwriting the TitanicSurvival you have now.
   
   This should be the same as last time, except you should use type = 'response' as an option.
  CorrectAnswer: TitanicSurvival <- TitanicSurvival %>% mutate(response_predict = predict(logit, type = 'response'))
  AnswerTests: cor(TitanicSurvival[['response_predict']],predict(logit, type = 'response')) > .99; expr_uses_func('mutate')
  Hint: predict(logit, type = 'response') will give you the predicted values. Just make that the right-hand-side part of a mutate command creating the variable response_predict, storing that new variable in TitanicSurvival.

- Class: cmd_question
  Output: |-
   We can see that it runs the index through its link function to get its prediction.
   
   The logit link function is exp(x)/(1+exp(x)).
   
   Let's create that as a function so we can check logit's work! Put in:
   
   loglink <- function(x) { exp(x)/(1+exp(x)) }
  CorrectAnswer: loglink <- function(x) { exp(x)/(1+exp(x)) }
  AnswerTests: omnitest(correctExpr='loglink <- function(x) { exp(x)/(1+exp(x)) }')
  Hint: Copy/paste the loglink function in.

- Class: cmd_question
  Output: |-
   See how easy it is to create our own function?
   
   Now, take TitanicSurvival$index_predict, send that to loglink(), and then get the correlation of that with TitanicSurvival$response_predict using cor(x,y). 
   
   We should get a pretty high (if not exactly perfect, due to rounding issues) correlation! That's how we get from one to the other.
  CorrectAnswer: TitanicSurvival %>% pull(index_predict) %>% loglink() %>% cor(TitanicSurvival$response_predict)
  AnswerTests: omnitest(correctVal=TitanicSurvival %>% pull(index_predict) %>% loglink() %>% cor(TitanicSurvival$response_predict))
  Hint: We want the cor(x,y) of two variables. Our x is loglink(TitanicSurvival$index_predict), and our y is TitanicSurvival$response_predict.

- Class: cmd_question
  Output: |-
   Now let's see the coefficients and (an imitation of) the marginal effects!
   
   Use group_by(sex) with
   
   summarize(index = mean(index_predict), resp = mean(response_predict))
   
   to get the average index and average predicted response by sex.
  CorrectAnswer: TitanicSurvival %>% group_by(sex) %>% summarize(index = mean(index_predict), resp = mean(response_predict))
  AnswerTests: check_equality_of_data(TitanicSurvival %>% group_by(sex) %>% summarize(index = mean(index_predict), resp = mean(response_predict)))
  Hint: We have TitanicSurvival, group_by(sex), and the summarize() function above. Link the three of them with pipes %>%

- Class: text
  Output: |-
   You can see the difference in the index between men and women of (-1.55 - 1.30) = -2.85, similar to the -2.5 coefficient (why isn't it exactly the same? Becuase this isn't actually how those coefficients come about, this is just an illustration).
   
   Similarly, you can see the difference in the predicted probability of (.205 - .753) = -552, similar to the -.49 we got with the linear probability model, and probably close to about what we'll get when we calculate marginal effects properly!
   
   Let's go ahead and do that.

- Class: cmd_question
  Output: |-
   Send logit to summary(marginaleffects()) from the marginaleffects package to get the marginal effects of each of the variables. Store the result as margfx.
   
   No options should be necessary. By default it will produce Average Marginal Effects (i.e. it will calculate the marginal effect for each person, since marginal effects vary based on the values of the independent variables) and average those. That's what we want! It's usually what we want.
  CorrectAnswer: margfx <- summary(marginaleffects(logit))
  AnswerTests: any_of_exprs('margfx <- summary(marginaleffects(logit))', 'margfx <- logit %>% marginaleffects() %>% summary()')
  Hint: Just put logit inside the marginaleffects() function and then put that inside of summary().

- Class: cmd_question
  Output: Look at margfx to see how it compares to our LPM results from before.
  CorrectAnswer: margfx
  AnswerTests: omnitest(correctExpr='margfx')
  Hint: Just type out margfx.

- Class: text
  Output: Looks like LPM and logit marginal effects give very similar answers in this case! Although do keep in mind that won't always be true.

- Class: text
  Output: |-
   The marginaleffects() command we did got us average marginal effects, but maybe we want the marginal effect *for a particular type of person*.
   
   We can use the datagrid() function to make a data set with variables set to whatever value we like.
   
   For example, marginaleffects(my_model, datagrid()) will give the marginal effects for my_model with the original data set entirely at its means.
   
   Or, marginaleffects(my_model, datagrid(X = 1, grid.type = 'counterfactual')) will give the average marginal effects as though everyone had their X value set to 1.

- Class: cmd_question
  Output: |-
   The marginaleffects() command we did got us average marginal effects, but maybe we want the marginal effect *for a particular type of person*.
   
   We can use the datagrid() function to make a data set with variables set to whatever value we like.
   
   For example, marginaleffects(my_model, datagrid()) will give the marginal effects for my_model with the original data set entirely at its means.
   
   Or, marginaleffects(my_model, datagrid(X = 1, grid.type = 'counterfactual')) will give the average marginal effects as though everyone had their X value set to 1. The grid.type = 'counterfactual' tells it to use the original data but with some values set counterfactually.
   
   Use summary(marginaleffects()) to calculate the marginal effects at age = 60.
   
   Note this will not calculate the marginal effects *among 60-year-olds*. Rather, it will calculate the marginal effect of each variable *as though everyone's index were what it would be if age were 60*. Our little newdata approach here should make that clear!
  CorrectAnswer: summary(marginaleffects(logit, datagrid(age = 60, grid.type = "counterfactual")))
  AnswerTests: any_of_exprs('summary(marginaleffects(logit, datagrid(age = 60, grid.type = "counterfactual")))', 'logit %>% marginaleffects(age = 60, grid.type = "counterfactual") %>% summary()')
  Hint: Do the same marginaleffects as last time, but add datagrid(age = 60, grid.type = "counterfactual").

- Class: cmd_question
  Output: |-
   We will close things out with some joint hypothesis tests!
   
   We will be testing if the passengerClass variable matters.
   
   First, we will use wald from fixest as we normally do.
   
   Use wald to test whether passengerClass2nd and passengerClass3rd are jointly 0 in logit. Remember, you can just give it 'passengerClass' and it will get all variables that start that way.
  CorrectAnswer: wald(logit, 'passengerClass')
  AnswerTests: omnitest(correctExpr='wald(logit, "passengerClass")')
  Hint: Use wald as normal. logit is the model, and we are testing 'passengerClass'.

- Class: mult_question
  Output: Based on this output, are the passengerClass binary variables jointly significant at the 1% level?
  AnswerChoices: Yes; No; No way to tell
  CorrectAnswer: Yes
  AnswerTests: omnitest(correctVal='Yes')
  Hint: Look at Pr(>Chisq) for the p-value. Remember, Xe-Y means X*(10^-Y).
